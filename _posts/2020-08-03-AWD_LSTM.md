# Visualizing Dropout Techniques in AWD-LSTM Network
## Motivation
As a tool for regularization, dropout has gained a tremendous success in applying on convolutional network. 

Such tool effectively controls the generalization power + data efficiency of convolution network with hugh parameters. However, little success has been gained when applying the same dropout techniques on recurrent network. 

Author propose a new regularization and optimization scheme for training language model with recurrent network. Why?
- naive dropout disrupt RNN capacity to learn long-term dependencies
- the implementations are compatible with NVIDIA's cuDNN implementation of LSTM/ RNN networks which shares significant speed up

based upon fastai's implementation. May have slight variation against author's original implementation, but the principle should align

## Overview
- DropConnect on hidden-to-hidden weight matrix
- Embedding dropout
- Activation dropout

## Out of Scope
- ASGD
- variable length back-propagation sequence
- Weight type (embedding weight shares with weight before softmax)
- Independent embedding size and hidden size
- Activation regularization
- Temporal activation regularization

## What is Dropout?
Dropout is a famous weapon for fighting against model overfitting. In essence, dropout is a binary mask applied on a layer (usually the layer applied is an activation layer). It randomly zeroes out each activation unit with a specified chance. By doing so it could effectively discourage model from relying on a small subset of neurons but instead utilize more of its neurons, and hence boosting the generalization power of a model. 

Dropout typically mask each activation unit independently. This setting has gained a dominant success in Convolution Neural Network but failed miserably in Recurrent Neural Network because such independence masking seriously hurts RNN from learning long-term dependency relationship. The various dropout techniques I introduce below are essentially to impose different dependency structure on the dropout mask so as to effectively regulate RNN.

```
insert CNN dropout visuals
```

## A Briefly Introduction Word Embedding
In language model, we usually have a fixed set of vocabulary and we represent each word by a vector. Word embedding is essentially a weight matrix storing all these vectors. Just imagine a word embedding is a compat storage with all vectors vertically packed so that its row is the vocabulary size and its col is the dimension of the vecotr. You could retrieve the vector of a word by its corresponding index. 

```
insert word embedding visuals
```

## Dropout on Word Embedding
Applying dropout on word embedding is anologous to disappearing some words from a vocabulary. To achieve such effect, we could randomly select some words (indexes) from the vocabulary and completely zero out their vectors. Technically, it is equivalent to randomly masking out some rows of the word embedding weight matrix. 

```
insert word embedding dropout visuals
```

To minimize the operation, such dropout mask only updates once for each mini-batch and it is applied on the word embedding before feed-forward pass. Another details is that the dropout mask on word embedding is not a binary mask. Instead, it scales non-mask entry by a factor of $1/(1-p)$ where $p$ is the probability that a word to be masked out. 

## A Brief Introduction to Recurrent Layer
RNN is composed of recurrent layer(s). There are many variants of RNN and each variant has its unique design of recurrent layer. For example, one of its variant Long Short Term Memory Network (LSTM) introduces forget gate, input gate and output gate in its recurrent layer. To avoid dwelling into those complexity, I will mainly focus on vanilla recurrent layer. 

The following introduce 2 types of dropout techniques for regulating recurrent layers.

## Dropout on Hidden States
- Variational dropout: lock dropout mask across time-step. Different example has different dropout mask
- Keep in mind that the same dropout principles could be applied in all variants of RNN

## Dropout on Hidden-to-Hidden Weight Matrix
- aka DropConnect
- prevent overfitting on recurrent connection
- applied before feed-forward and back-propagation stage. 
- the same masked weights are reused across all timesteps

## Putting All Dropout Techniques Together


## Closing Remark
At last, I hope this article could help clear most of the subtleties in the dropout schemes from AWD-LSTM Network. 

## Reference