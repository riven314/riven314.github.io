# Illustrating Dropout Techniques in AWD-LSTM Network

## Motivation
As a tool for regularization, dropout has gained a tremendous success in applying on convolutional network. 

Such tool effectively controls the generalization power + data efficiency of convolution network with hugh parameters. However, little success has been gained when applying the same dropout techniques on recurrent network. 

Author propose a new regularization and optimization scheme for training language model with recurrent network. Why?
- naive dropout disrupt RNN capacity to learn long-term dependencies
- the implementations are compatible with NVIDIA's cuDNN implementation of LSTM/ RNN networks which shares significant speed up

based upon fastai's implementation. May have slight variation against author's original implementation, but the principle should align

## Overview
- DropConnect on hidden-to-hidden weight matrix
- Embedding dropout
- Activation dropout

## Out of Scope
- ASGD
- variable length back-propagation sequence
- Weight type (embedding weight shares with weight before softmax)
- Independent embedding size and hidden size
- Activation regularization
- Temporal activation regularization

## What is Dropout?
Dropout is a famous weapon for fighting against model overfitting. In essence, dropout is a binary mask applied on a layer (usually the layer applied is an activation layer). It randomly zeroes out each activation unit with a specified chance. By doing so it could effectively discourage model from relying on a small subset of neurons but instead utilize more of its neurons, and hence boosting the generalization power of a model. 

Dropout typically mask each activation unit independently. This setting has gained a dominant success in Convolution Neural Network but failed miserably in Recurrent Neural Network because such independence masking seriously hurts RNN from learning long-term dependency relationship. The various dropout techniques I introduce below are essentially to impose different dependency structure on the dropout mask so as to effectively regulate RNN.

```
insert CNN dropout visuals
```

## A Briefly Introduction Word Embedding
In language model, we usually have a fixed set of vocabulary and we represent each word by a vector. Word embedding is essentially a weight matrix storing all these vectors. Just imagine a word embedding is a compat storage with all vectors vertically packed so that its row is the vocabulary size and its col is the dimension of the vecotr. You could retrieve the vector of a word by its corresponding index. 

```
insert word embedding visuals
```

## Dropout on Word Embedding
Applying dropout on word embedding is anologous to disappearing some words from a vocabulary. To achieve such effect, we could randomly select some words (indexes) from the vocabulary and completely zero out their vectors. Technically, it is equivalent to randomly masking out some rows of the word embedding weight matrix. 

```
insert word embedding dropout visuals
```

To minimize the operation, such dropout mask only updates once for each mini-batch and it is applied on the word embedding before feed-forward pass. Another details is that the dropout mask on word embedding is not a binary mask. Instead, it scales non-masked weight by a factor of $1/(1-p)$ where $p$ is the probability that a word to be masked out. In other words, a higher $p$ leads to a larger scaling factor on non-masked weight.

## A Brief Introduction to Recurrent Layer
Recurrent Neural Network is a composition of recurrent layer(s). There are many variants of RNN and each variant has its unique design of recurrent layer. For example, one of its variant Long Short Term Memory Network (LSTM) introduces forget, input, cell and output gates in its recurrent layer for better performance. To avoid dwelling into those complexity, I will mainly focus on vanilla recurrent layer, but keep in mind that the dropout principles introduced here can be applied to all RNN variants. Also note that the notations used below mainly follow [PyTorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html).

Recurrent layer is designed to handle sequence data. In contrast to image data, sequence data usually have serial dependency and they have variable length. Recurrent layer processes sequence data across timestep using its shared parameters. Such design is an efficient structure for model parameterization (less parameters to be learned) and capturing essential dependency (memory).

In order to store the essential memory from the preceding timesteps, recurrent layer introduces an activation layer called hidden state $\bold{h_{t-1}}$. At each time step $t$, a recurrent layer receives input vector at current timestep $\bold{x_{t}}$ and hidden state at preceding timestep $\bold{h_{t-1}}$ to generate hidden state at current timestep $\bold{h_t}$. $\bold{h_t}$ is the output of the layer and also be passed to the next timestep as an recurrent input. 

```
insert RNN diagram overview
```

Note that $\bold{x_{t}}$ and $\bold{h_{t-1}}$ often have different dimension. To abbreviate, lets denote the input vector's dimension as _input_size_ and hidden state's dimension as _hidden_size_. Mathematically underneath the hood of the recurrent layer, $\bold{x_{t}}$ and $\bold{h_{t-1}}$ are undergoing the following transformation to get $\bold{h_t}$:

$$
\bold{h_{t}} = f(\bold{W_{ih} x_{t} + W_{hh} h_{t-1} + b_{ih} + b_{hh}})
$$

We call $\bold{W_{ih}}$ as input-to-hidden weight matrix because it helps transform an input vector into a vector of size _hidden_size_. By the same token, we call $\bold{W_{hh}}$ as hidden-to-hidden weight matrix. Activation function $f$ is usually a hyperbolic tangent function $tanh$.

```
insert inner mechanism of Recurrent layer
```

The above overview should suffice to introduce the remaining 2 types of dropout techniques for regulating recurrent layers. 

## Dropout on Hidden State
An intuitive way to regulate recurrent layer is to apply dropout on hidden state. However, there are several caveats we need to notice when doing so:  
1. Only hidden state output has dropout applied
2. For different samples in a mini-batch, they should have different dropout masks applied on their hidden state
3. For different timesteps of a given sample, the same dropout mask must be applied on its hidden state
4. In case of multiple recurrent layers, different dropout masks are applied on different hidden states for a given sample
5. In case of multiple recurrent layers, the hidden state output from the final layer does not have dropout applied

Similar to dropout on word embedding, the dropout masks on hidden state only update once for each mini-batch, before feed-forward pass.

Here are some simple diagrams to illustrate the above details:
```
several diagrams to illustrate dropout on hidden states
```

## Dropout on Hidden-to-Hidden Weight Matrix
- aka DropConnect
- prevent overfitting on recurrent connection
- applied before feed-forward and back-propagation stage. 
- the same masked weights are reused across all timesteps

## Putting All Dropout Techniques Together
- same masking across timestep?
- same masking across samples in the mini-batch?
- 

## Closing Remark
At last, I hope this article could help clear most of the subtleties in the dropout schemes from AWD-LSTM Network. 

## Reference