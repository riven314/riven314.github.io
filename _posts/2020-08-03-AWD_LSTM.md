Visualizing the Dropout Schemes of ASG Weighted-Dropped LSTM Network (AWD-LSTM)

## Motivation
As a tool for regularization, dropout has gained a tremendous success in applying on convolutional network. 

Such tool effectively controls the generalization power + data efficiency of convolution network with hugh parameters. However, little success has been gained when applying the same dropout techniques on recurrent network. 

Author propose a new regularization and optimization scheme for training language model with recurrent network. Why?
- naive dropout disrupt RNN capacity to learn long-term dependencies
- the implementations are compatible with NVIDIA's cuDNN implementation of LSTM/ RNN networks which shares significant speed up

based upon fastai's implementation. May have slight variation against author's original implementation, but the principle should align

## Overview
- DropConnect on hidden-to-hidden weight matrix
- Embedding dropout
- Activation dropout

## Out of Scope
- ASGD
- variable length back-propagation sequence
- Weight type (embedding weight shares with weight before softmax)
- Independent embedding size and hidden size
- Activation regularization
- Temporal activation regularization

## Dropout on Embedding
- micmic a specific word disappears within a mini-batch
- scaled by 1/(1-p)

## Dropout on Hidden States
- Variational dropout: lock dropout mask across time-step. Different example has different dropout mask


## Dropout on Weight Matrix
- aka DropConnect
- prevent overfitting on recurrent connection
- applied before feed-forward and back-propagation stage. 
- the same masked weights are reused across all timesteps


## Evaluation
The proposed scheme gives a significant boost to the perplexity of the generated sentences. 

## Closing Remark


## Reference