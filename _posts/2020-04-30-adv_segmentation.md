# [Paper Review] Adversarial Learning for Semi-Supervised Semantic Segmentation 

### Summary
Deeep supervised learning has recently gained a significant success on a lot of computer vision problems, such as image classification and segmentation. However, it comes as a tremendous cost of human annotations. While modern industry has evolved to have specialized service for this, acquiring such high volume of labelled data is still expensive in cost and time. Such background motivates the advance of semi-supervised learning, where one utilize both labelled and unlabelled data to train a deep learning model. Having said such advancement, most related studies have been revolved around image classification instead of object detection and image segmentation, both of which require way higher cost to acquire labelled data. This paper is one of the few (and also a highly cited one) that aims to apply semi-supervised learning on image segmentation. 

### Methodology
The author frame the whole problem as an adversarial training with both labelled data and unlabelled data. 

### Model Architecture


### Tricks Applied in Training


### Results


### Personal Thought
How one could train a supervised learning model with limited annotation cost is one of the problem I recently think about. I believe it is also one of the most practical issues one has to face in the industry. Instead of image classification, I am more interested in how it could be applied in object detetion and image segmentation. While the paper has addressed a lot of technicality on semi-supervised training, it doesn't address how human could augment with machine to do data annotation more efficiently. But I think one of its module could potentially be a useful tool for this direction and that module is also what draws my attention on this paper -- The confidence map generated by discriminator. 

We know not all data points are equal -- some have high value to the model to learn than the other. When we are faced with limited annotation resource and have to decide what samples we should annotate, we are considering what samples could drive a big enough marginal improvement to the model. One useful indicator is the prediction uncertainty of the trained model. If we have a way to estimate such certainty and we learn that a model has high certainty on predicting a sample, it indicates that the sample has some features that contradicts with the model's or the model failed to generalized. Including such sample in our labelled set could marginally enhance the generalization power of the model. In related paper, it is usually referred as "enhancing the decision boundary of the model".


### Reference